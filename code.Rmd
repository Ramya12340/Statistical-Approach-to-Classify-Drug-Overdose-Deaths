
```{r}

library(data.table)  
library(caret)      
library(randomForest)
library(gbm)
library(e1071)       

df <- fread("~/Downloads/Drug_overdose_death_rates__by_drug_type__sex__age__race__and_Hispanic_origin__United_States.csv")

summary(df)
```

```{r}
panel_counts <- table(df$PANEL)

panel_percentages <- round(100 * panel_counts / sum(panel_counts), 1)

panel_labels <- paste(names(panel_counts), " \n ", panel_percentages, "%", sep="")

pie(panel_counts, main = "Distribution of PANEL Categories", col = rainbow(length(panel_counts)), labels = panel_labels, cex = 0.5)

```
```{r}
hist(df$ESTIMATE)
```

```{r}
selected_data <- df[, c("UNIT_NUM", "STUB_NAME_NUM", "STUB_LABEL_NUM", "YEAR_NUM", "AGE_NUM", "ESTIMATE")]

str(selected_data)
```

```{r}
mean_estimate <- mean(selected_data$ESTIMATE, na.rm = TRUE)

selected_data$ESTIMATE[is.na(selected_data$ESTIMATE)] <- mean_estimate

selected_data$ESTIMATE <- (selected_data$ESTIMATE - min(selected_data$ESTIMATE)) / (max(selected_data$ESTIMATE) - min(selected_data$ESTIMATE))

summary(selected_data$ESTIMATE)
```



```{r}
full_data <- cbind(selected_data, PANEL_NUM = df$PANEL_NUM)

full_data$PANEL_NUM <- as.factor(full_data$PANEL_NUM)

full_data$UNIT_NUM <- as.factor(full_data$UNIT_NUM)
full_data$STUB_NAME_NUM <- as.factor(full_data$STUB_NAME_NUM)
full_data$STUB_LABEL_NUM <- as.factor(full_data$STUB_LABEL_NUM)
full_data$YEAR_NUM <- as.factor(full_data$YEAR_NUM)
full_data$AGE_NUM <- as.factor(full_data$AGE_NUM)

str(full_data)
```

```{r}
set.seed(123)  # for reproducibility
trainIndex <- createDataPartition(full_data$PANEL_NUM, p = 0.8, 
                                  list = FALSE, 
                                  times = 1)

train_data <- full_data[trainIndex, ]
test_data <- full_data[-trainIndex, ]

dim(train_data)
dim(test_data)
```

```{r}
library(randomForest)

set.seed(123)  # for reproducibility
rf_model <- randomForest(PANEL_NUM ~ ., data = train_data, ntree = 500, mtry = 3, importance = TRUE)

print(rf_model)

varImpPlot(rf_model)
```
```{r}

predictions <- predict(rf_model, test_data)

confusionMatrix <- confusionMatrix(as.factor(predictions), as.factor(test_data$PANEL_NUM))

print(confusionMatrix$table)

accuracy <- confusionMatrix$overall['Accuracy']
print(paste("Accuracy:", accuracy))
```
```{r}
conf_matrix <- matrix(c(199, 26, 1, 0, 2, 0,
                        8, 144, 26, 2, 8, 9,
                        0, 34, 99, 27, 14, 26,
                        0, 5, 31, 70, 66, 78,
                        0, 2, 16, 61, 59, 55,
                        0, 2, 38, 47, 58, 39),
                      nrow = 6, byrow = TRUE)

precision <- diag(conf_matrix) / rowSums(conf_matrix)
recall <- diag(conf_matrix) / colSums(conf_matrix)
f1_score <- 2 * (precision * recall) / (precision + recall)

cat("Precision per class:\n")
print(precision)
cat("Recall per class:\n")
print(recall)
cat("F1-Score per class:\n")
print(f1_score)
```

```{r}
library(pROC)

rf_probabilities <- predict(rf_model, test_data, type = "prob")

par(mfrow=c(3,2))  

for (i in levels(test_data$PANEL_NUM)) {
  
    response_binary <- factor(ifelse(test_data$PANEL_NUM == i, "positive", "negative"),
                              levels = c("negative", "positive"))

    
    roc_curve <- roc(response = response_binary, predictor = rf_probabilities[, i],
                     levels = c("negative", "positive"), direction = "<")


    plot(roc_curve, main = paste("ROC for Class", i))
    
    auc_value <- auc(roc_curve)
text(x = 0.8, y = 0.2, labels = paste("AUC:", round(auc_value, 3)),
         col = "red", cex = 0.8, font = 2)}

```

```{r}
library(nnet)
library(gbm)
library(rpart)

multinom_model <- multinom(PANEL_NUM ~ ., data = train_data)

multinom_predictions <- predict(multinom_model, newdata = test_data)

multinom_results <- confusionMatrix(as.factor(multinom_predictions), as.factor(test_data$PANEL_NUM))
print(multinom_results)
```
```{r}

conf_matrix <- matrix(c(144, 45, 17, 36, 48, 44,
                        25,  74,  12 , 19 , 27,  30,
                        11,  15,  11,  34,  22,  30,
                        16,  44 , 79,  62,  77,  67,
                         3,  16,  42,  29,  10,  28,
                        8,  13,  46,  27,  23,   8),
                      nrow = 6, byrow = TRUE)
precision <- diag(conf_matrix) / rowSums(conf_matrix)
recall <- diag(conf_matrix) / colSums(conf_matrix)
f1_score <- 2 * (precision * recall) / (precision + recall)

cat("Precision per class:\n")
print(precision)
cat("Recall per class:\n")
print(recall)
cat("F1-Score per class:\n")
print(f1_score)
```

```{r}
log_probabilities <- predict(multinom_model, test_data, type = "prob")

par(mfrow=c(3,2))  

for (i in levels(test_data$PANEL_NUM)) {
  
    response_binary <- factor(ifelse(test_data$PANEL_NUM == i, "positive", "negative"),
                              levels = c("negative", "positive"))

    
    roc_curve <- roc(response = response_binary, predictor = log_probabilities[, i],
                     levels = c("negative", "positive"), direction = "<")


    plot(roc_curve, main = paste("ROC for Class", i))
    
    auc_value <- auc(roc_curve)
text(x = 0.8, y = 0.2, labels = paste("AUC:", round(auc_value, 3)),
         col = "red", cex = 0.8, font = 2)}
```

```{r}
library(e1071)

naive_bayes_model <- naiveBayes(PANEL_NUM ~ ., data = train_data, laplace = 1)

naive_bayes_predictions <- predict(naive_bayes_model, newdata = test_data)

naive_bayes_results <- confusionMatrix(as.factor(naive_bayes_predictions), as.factor(test_data$PANEL_NUM))

print(naive_bayes_results)

```
```{r}
conf_matrix <- matrix(c(81, 36, 0, 0, 6, 5,
                        42,  34,   5,   0,   4,   5,
                        24,  32,  17,  52,  57,  68,
                        27,  69, 118, 106,  87,  93,
                         8,  11,  13,   5,   0,  10,
                        25,  25,  54,  44,  53,  26),
                      nrow = 6, byrow = TRUE)

precision <- diag(conf_matrix) / rowSums(conf_matrix)
recall <- diag(conf_matrix) / colSums(conf_matrix)
f1_score <- 2 * (precision * recall) / (precision + recall)

cat("Precision per class:\n")
print(precision)
cat("Recall per class:\n")
print(recall)
cat("F1-Score per class:\n")
print(f1_score)
```
```{r}
nb_probabilities <- predict(naive_bayes_model, test_data, type = "raw")

par(mfrow=c(3,2))  

for (i in levels(test_data$PANEL_NUM)) {
  
    response_binary <- factor(ifelse(test_data$PANEL_NUM == i, "positive", "negative"),
                              levels = c("negative", "positive"))

    
    roc_curve <- roc(response = response_binary, predictor = nb_probabilities[, i],
                     levels = c("negative", "positive"), direction = "<")


    plot(roc_curve, main = paste("ROC for Class", i))
    
    auc_value <- auc(roc_curve)
text(x = 0.8, y = 0.2, labels = paste("AUC:", round(auc_value, 3)),
         col = "red", cex = 0.8, font = 2)}
```
 

```{r}
library(e1071)
svm_model <- svm(PANEL_NUM ~ ., data = train_data, kernel = "radial", cost = 10, scale = TRUE, probability = TRUE)

summary(svm_model)

```

```{r}
svm_predictions <- predict(svm_model, newdata = test_data)

svm_results <- confusionMatrix(as.factor(svm_predictions), as.factor(test_data$PANEL_NUM))
print(svm_results)
```
```{r}
conf_matrix <- matrix(c(149, 22, 17, 43, 50, 53,
                        21,  99,   12,   14,   26,   19,
                        3,  16,  16,  21,  13,  23,
                        23,  52, 76, 70,  83,  72,
                         3,  10,  26,   35,   11,  25,
                        8,  8,  60,  24,  24,  15),
                      nrow = 6, byrow = TRUE)

precision <- diag(conf_matrix) / rowSums(conf_matrix)
recall <- diag(conf_matrix) / colSums(conf_matrix)
f1_score <- 2 * (precision * recall) / (precision + recall)

cat("Precision per class:\n")
print(precision)
cat("Recall per class:\n")
print(recall)
cat("F1-Score per class:\n")
print(f1_score)
```

```{r}
svm_probabilities <- attr(predict(svm_model, test_data, probability = TRUE), "probabilities")
par(mfrow=c(3,2))  

for (i in levels(test_data$PANEL_NUM)) {
  
    response_binary <- factor(ifelse(test_data$PANEL_NUM == i, "positive", "negative"),
                              levels = c("negative", "positive"))

    
    roc_curve <- roc(response = response_binary, predictor = svm_probabilities[, i],
                     levels = c("negative", "positive"), direction = "<")


    plot(roc_curve, main = paste("ROC for Class", i))
    
    auc_value <- auc(roc_curve)
text(x = 0.8, y = 0.2, labels = paste("AUC:", round(auc_value, 3)),
         col = "red", cex = 0.8, font = 2)}
```

```{r}
gbm_model <- gbm(PANEL_NUM ~ ., data = train_data, distribution = "multinomial", n.trees = 150, interaction.depth = 3, shrinkage = 0.1, cv.folds = 5, n.minobsinnode = 10, verbose = FALSE)

gbm_predictions <- predict(gbm_model, newdata = test_data, n.trees = gbm_model$n.trees, type = "response")

gbm_predictions <- apply(gbm_predictions, 1, which.max)

gbm_predictions <- factor(gbm_predictions, levels = 1:length(levels(train_data$PANEL_NUM)), labels = levels(train_data$PANEL_NUM))

gbm_results <- confusionMatrix(gbm_predictions, as.factor(test_data$PANEL_NUM))
print(gbm_results)

```

```{r}
conf_matrix <- matrix(c(196, 30, 3, 0, 1, 1,
                        10,  110,   39,   5,   17,   9,
                        1,  54,  63,  23,  12,  33,
                        0,  5, 36, 61,  62,  86,
                         0,  2,  19,   61,   53,  44,
                        0,  6,  47,  57,  62,  34),
                      nrow = 6, byrow = TRUE)

precision <- diag(conf_matrix) / rowSums(conf_matrix)
recall <- diag(conf_matrix) / colSums(conf_matrix)
f1_score <- 2 * (precision * recall) / (precision + recall)

cat("Precision per class:\n")
print(precision)
cat("Recall per class:\n")
print(recall)
cat("F1-Score per class:\n")
print(f1_score)
```

```{r}
gbm_probabilities <- predict(gbm_model, test_data, type = "response", n.trees = gbm_model$n.trees)
gbm_probabilities <- array(gbm_probabilities, dim = c(dim(gbm_probabilities)[1], dim(gbm_probabilities)[2]))
par(mfrow=c(3,2))  

for (i in 1:ncol(gbm_probabilities)) {
    response_binary <- factor(ifelse(test_data$PANEL_NUM == levels(test_data$PANEL_NUM)[i], "positive", "negative"),
                              levels = c("negative", "positive"))

    roc_curve <- roc(response = response_binary, predictor = gbm_probabilities[, i],
                     levels = c("negative", "positive"), direction = "<")

    plot(roc_curve, main = paste("ROC for Class", levels(test_data$PANEL_NUM)[i]))
    
    auc_value <- auc(roc_curve)
    
    text(x = 0.8, y = 0.2, labels = paste("AUC:", round(auc_value, 3)),
         col = "red", cex = 0.8, font = 2)
}

```

```{r}
library(dplyr)
library(ggplot2)

metrics_data <- data.frame(
  Model = rep(c("Logistic", "SVM", "Naive Bayes", "Random Forest", "GBM"), each = 18),
  Class = rep(paste("Class", 0:5), times = 5),
  Metric = rep(c("Precision", "Recall", "F1-Score"), each = 6, times = 5),
  Value = c(
    # Logistic
    0.43113772, 0.39572193, 0.08943089, 0.17971014, 0.07812500, 0.06400000,
    0.69565217, 0.35748792, 0.05314010, 0.29951691, 0.04830918, 0.03864734,
    0.53234750, 0.37563452, 0.06666667, 0.22463768, 0.05970149, 0.04819277,
    # SVM
    0.4461078, 0.5183246, 0.1739130, 0.1861702, 0.1000000, 0.1079137,
    0.71980676, 0.47826087, 0.07729469, 0.33816425, 0.05314010, 0.07246377,
    0.55083179, 0.49748744, 0.10702341, 0.24013722, 0.06940063, 0.08670520,
    # Naive Bayes
    0.6328125, 0.3777778, 0.0680000, 0.2120000, 0.0000000, 0.1145374,
    0.3913043, 0.1642512, 0.0821256, 0.5120773, 0.0000000, 0.1256039,
    0.48358209, 0.22895623, 0.07439825, 0.29985856, 0.0000, 0.1198156,
    # Random Forest
    0.8728070, 0.7309645, 0.4950000, 0.2800000, 0.3056995, 0.2119565,
    0.9613527, 0.6760563, 0.4691943, 0.3381643, 0.2850242, 0.1884058,
    0.9149425, 0.7024390, 0.4817518, 0.3063457, 0.2950000, 0.1994885,
    # GBM
    0.8484848, 0.5789474, 0.3387097, 0.2440000, 0.2960894, 0.1650485,
    0.9468599, 0.5314010, 0.3043478, 0.2946860, 0.2560386, 0.1642512,
    0.8949772, 0.5541562, 0.3206107, 0.2669584, 0.2746114, 0.1646489
  )
)

```
```{r}
ggplot(metrics_data, aes(x = Class, y = Value, fill = Model)) +
  geom_bar(stat = "identity", position = position_dodge(), width = 0.7) +
  facet_wrap(~ Metric, scales = "free_y") +
  labs(title = "Model Performance Comparison Across Metrics",
       x = "Class",
       y = "Value") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  scale_fill_brewer(palette = "Set1")  

```



